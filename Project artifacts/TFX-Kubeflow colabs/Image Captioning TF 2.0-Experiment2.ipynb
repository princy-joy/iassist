{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Attention in Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook modifies the [Image Captioning with Attention Tensorflow 2.0 notebook](https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/image_captioning.ipynb)\n",
    "to work with kubeflow pipelines.  This pipeline creates a model that can caption an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running notebook:\n",
    "Make sure you completed the setup instructions in the README (including creating the base image).\n",
    "\n",
    "### Install Kubeflow pipelines\n",
    "Install the `kfp` package if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (11.0.0)\n",
      "Requirement already satisfied: tabulate in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp) (1.6.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.1.7)\n",
      "Requirement already satisfied: fire>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: Deprecated in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.2.12)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.38.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.30.0)\n",
      "Requirement already satisfied: strip-hints in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.5.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire>=0.3.1->kfp) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (49.6.0.post20210108)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.7.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.2.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.6.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (1.26.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (1.53.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (3.15.8)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (20.9)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (2021.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.20)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (0.17.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2020.12.5)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp) (2.10)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp) (0.36.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate service account credentials\n",
    "This allows for using `gsutil` from the notebook to upload the dataset to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [iassist-deep-dreamers@deep-dreamers-iassist.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth activate-service-account --key-file=\"deep-dreamers-iassist-004252d99604.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and upload to GCS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to download the [MS COCO dataset](http://cocodataset.org/#download).  This sample uses both the 2014 train images and 2014 train/val annotations.  The following cells download a small subset (<1000 imgs) of the dataset and the annotations to the GCS bucket specified below with `GCS_DATASET_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location to download dataset and put onto GCS (should be associated\n",
    "# with Kubeflow project)\n",
    "GCS_BUCKET = 'gs://kubeflowpipelines-default'\n",
    "GCS_DATASET_PATH = GCS_BUCKET + '/ms-coco'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images\n",
    "Downloads images to `${GCS_DATASET_PATH}/train2014/train2014`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-07 06:32:40--  http://images.cocodataset.org/zips/train2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.41.156\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.41.156|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13510573713 (13G) [application/zip]\n",
      "Saving to: ‘train2014.zip’\n",
      "\n",
      "train2014.zip       100%[===================>]  12.58G  60.8MB/s    in 2m 51s  \n",
      "\n",
      "2021-05-07 06:35:32 (75.1 MB/s) - ‘train2014.zip’ saved [13510573713/13510573713]\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file://./train2014/COCO_train2014_000000001099.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000003899.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000001999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000005099.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000016099.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000005699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000003999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000006599.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000009599.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000009999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000008999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000010799.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000011299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000017399.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000012999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000009199.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000018099.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000017799.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000015399.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000018299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000019399.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000019499.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000019899.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000020599.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000021599.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000022799.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000022899.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000023199.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000023699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000024699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000024299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000024899.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000025799.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000027599.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000026699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000027299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000028099.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000029799.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000030299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000029299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000030699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000032699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000035299.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000033799.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000037999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000038899.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000038999.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000039099.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000040699.jpg [Content-Type=image/jpeg]...\n",
      "Copying file://./train2014/COCO_train2014_000000040399.jpg [Content-Type=image/jpeg]...\n",
      "- [50/50 files][  9.1 MiB/  9.1 MiB] 100% Done                                  \n",
      "Operation completed over 50 objects/9.1 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Tutorial instructions\n",
    "# Download images (use -x to ignore ~99% of images)\n",
    "#gsutil -m rsync -x \".*0\\.jpg|.*1\\.jpg|.*2\\.jpg|.*3\\.jpg|.*4\\.jpg|.*5\\.jpg|.*6\\.jpg|.*7\\.jpg|.*8\\.jpg|.*09\\.jpg|.*19\\.jpg|.*29\\.jpg|.*39\\.jpg|.*49\\.jpg|.*59\\.jpg|.*69\\.jpg|.*79\\.jpg|.*89\\.jpg\" gs://images.cocodataset.org/train2014 {GCS_DATASET_PATH}/train2014/train2014\n",
    "\n",
    "# Modified instructions\n",
    "!wget http://images.cocodataset.org/zips/train2014.zip\n",
    "!unzip -q train2014.zip\n",
    "!gsutil -m rsync -x \".*0\\.jpg|.*1\\.jpg|.*2\\.jpg|.*3\\.jpg|.*4\\.jpg|.*5\\.jpg|.*6\\.jpg|.*7\\.jpg|.*8\\.jpg|.*09\\.jpg|.*19\\.jpg|.*29\\.jpg|.*39\\.jpg|.*49\\.jpg|.*59\\.jpg|.*69\\.jpg|.*79\\.jpg|.*89\\.jpg\" ./train2014 gs://kubeflowpipelines-default/ms-coco/train2014/train2014\n",
    "!rm -rf train2014 train2014.zip\n",
    "# To download the entire dataset uncomment and use the following command instead\n",
    "# !gsutil -m rsync gs://images.cocodataset.org/train2014 {GCS_DATASET_PATH}/train2014/train2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download annotations\n",
    "For some reason MS COCO blocks using `gsutil` with the annotations (GitHub issue [here](https://github.com/cocodataset/cocoapi/issues/216)).  You can work around this by downloading it locally, and then uploading it to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-07 06:46:05--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.109.3\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.109.3|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252872794 (241M) [application/zip]\n",
      "Saving to: ‘annotations_trainval2014.zip’\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.16M  81.9MB/s    in 2.9s    \n",
      "\n",
      "2021-05-07 06:46:08 (81.9 MB/s) - ‘annotations_trainval2014.zip’ saved [252872794/252872794]\n",
      "\n",
      "Archive:  annotations_trainval2014.zip\n",
      "  inflating: annotations_trainval2014/annotations/instances_train2014.json  \n",
      "  inflating: annotations_trainval2014/annotations/instances_val2014.json  \n",
      "  inflating: annotations_trainval2014/annotations/person_keypoints_train2014.json  \n",
      "  inflating: annotations_trainval2014/annotations/person_keypoints_val2014.json  \n",
      "  inflating: annotations_trainval2014/annotations/captions_train2014.json  \n",
      "  inflating: annotations_trainval2014/annotations/captions_val2014.json  \n",
      "Copying file://annotations_trainval2014/annotations/instances_train2014.json [Content-Type=application/json]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://annotations_trainval2014/annotations/person_keypoints_train2014.json [Content-Type=application/json]...\n",
      "Copying file://annotations_trainval2014/annotations/instances_val2014.json [Content-Type=application/json]...\n",
      "Copying file://annotations_trainval2014/annotations/captions_train2014.json [Content-Type=application/json]...\n",
      "Copying file://annotations_trainval2014/annotations/person_keypoints_val2014.json [Content-Type=application/json]...\n",
      "Copying file://annotations_trainval2014/annotations/captions_val2014.json [Content-Type=application/json]...\n",
      "| [6/6 files][805.7 MiB/805.7 MiB] 100% Done                                    \n",
      "Operation completed over 6 objects/805.7 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Download to local, upload to GCS, then delete local download\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "!unzip annotations_trainval2014.zip -d annotations_trainval2014\n",
    "!gsutil -m cp -r annotations_trainval2014 {GCS_DATASET_PATH}\n",
    "!rm -r annotations_trainval2014\n",
    "!rm annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup project info and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubeflow project settings\n",
    "PROJECT_NAME = 'iassist-deepdreamers' \n",
    "PIPELINE_STORAGE_PATH = GCS_BUCKET + '/ms-coco/components' # path to save pipeline component images\n",
    "BASE_IMAGE = 'gcr.io/%s/img-cap:latest' % PROJECT_NAME # using image created in README instructions\n",
    "\n",
    "# Target images for creating components\n",
    "PREPROCESS_IMG = 'gcr.io/%s/ms-coco/preprocess:latest' % PROJECT_NAME\n",
    "TOKENIZE_IMG = 'gcr.io/%s/ms-coco/tokenize:latest' % PROJECT_NAME\n",
    "TRAIN_IMG = 'gcr.io/%s/ms-coco/train:latest' % PROJECT_NAME\n",
    "PREDICT_IMG = 'gcr.io/%s/ms-coco/predict:latest' % PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing component\n",
    "This component takes `num_examples` images from `dataset_path` and feeds them through the deep CNN inceptionV3 (without the head).  The model outputs a tensor of shape `(64 x 2048)` that represents (2048) features obtained after dividing the image into an 8x8 (64) grid. The resulting model outputs are stored in `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) python_component. (This decorator does not seem to be used, so we deprecate it. If you need this decorator, please create an issue at https://github.com/kubeflow/pipelines/issues) -- Deprecated since version 0.2.6.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "@dsl.python_component(\n",
    "    name='img_data_preprocessing',\n",
    "    description='preprocesses images with inceptionV3',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def preprocess(dataset_path: str, num_examples: int, OUTPUT_DIR: str, \n",
    "        batch_size: int) -> str:\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    if OUTPUT_DIR == 'default':\n",
    "        OUTPUT_DIR = dataset_path + '/preprocess/'\n",
    "    \n",
    "    annotation_file = dataset_path + '/annotations_trainval2014/annotations/captions_train2014.json'\n",
    "    PATH = dataset_path + '/train2014/train2014/'\n",
    "    files_downloaded = tf.io.gfile.listdir(PATH)\n",
    "    \n",
    "    # Read the json file (CHANGE open() TO file_io.FileIO to use GCS)\n",
    "    with file_io.FileIO(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Store captions and image names in vectors\n",
    "    all_captions = []\n",
    "    all_img_name_vector = []\n",
    "    \n",
    "    print('Determining which images are in storage...')\n",
    "    for annot in annotations['annotations']:\n",
    "        caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "        image_id = annot['image_id']\n",
    "        img_name = 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "        full_coco_image_path = PATH + img_name\n",
    "        \n",
    "        if img_name in files_downloaded: # Only have subset\n",
    "            all_img_name_vector.append(full_coco_image_path)\n",
    "            all_captions.append(caption)\n",
    "\n",
    "    # Shuffle captions and image_names together\n",
    "    train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                              all_img_name_vector,\n",
    "                                              random_state=1)\n",
    "\n",
    "    # Select the first num_examples captions/imgs from the shuffled set\n",
    "    train_captions = train_captions[:num_examples]\n",
    "    img_name_vector = img_name_vector[:num_examples]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Preprocess the images before feeding into inceptionV3\n",
    "    def load_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path\n",
    "    \n",
    "    # Create model for processing images \n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "    \n",
    "    # Save extracted features in GCS\n",
    "    print('Extracting features from images...')\n",
    "    \n",
    "    # Get unique images\n",
    "    encode_train = sorted(set(img_name_vector))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "    image_dataset = image_dataset.map(\n",
    "        load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size)\n",
    "    \n",
    "    for img, path in image_dataset:\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            \n",
    "            # Save to a different location and as numpy array\n",
    "            path_of_feature = path_of_feature.replace('.jpg', '.npy')\n",
    "            path_of_feature = path_of_feature.replace(PATH, OUTPUT_DIR)\n",
    "            np.save(file_io.FileIO(path_of_feature, 'w'), bf.numpy())\n",
    "    \n",
    "    # Create array for locations of preprocessed images\n",
    "    preprocessed_imgs = [img.replace('.jpg', '.npy') for img in img_name_vector]\n",
    "    preprocessed_imgs = [img.replace(PATH, OUTPUT_DIR) for img in preprocessed_imgs]\n",
    "    \n",
    "    # Save train_captions and preprocessed_imgs to file\n",
    "    train_cap_path = OUTPUT_DIR + 'train_captions.npy' # array of captions\n",
    "    preprocessed_imgs_path = OUTPUT_DIR + 'preprocessed_imgs.py'# array of paths to preprocessed images\n",
    "    \n",
    "    train_captions = np.array(train_captions)\n",
    "    np.save(file_io.FileIO(train_cap_path, 'w'), train_captions)\n",
    "    \n",
    "    preprocessed_imgs = np.array(preprocessed_imgs)\n",
    "    np.save(file_io.FileIO(preprocessed_imgs_path, 'w'), preprocessed_imgs)\n",
    "    \n",
    "    return (train_cap_path, preprocessed_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-10 15:10:46:INFO:Build an image that is based on gcr.io/iassist-deepdreamers/img-cap:latest and push the image to gcr.io/iassist-deepdreamers/ms-coco/preprocess:latest\n",
      "2021-05-10 15:10:46:INFO:Building and pushing container image.\n",
      "2021-05-10 15:10:46:INFO:Generate build files.\n",
      "2021-05-10 15:10:47:INFO:Start a kaniko job for build.\n",
      "2021-05-10 15:10:47:INFO:Cannot find in-cluster config, trying the local kubernetes config. \n",
      "2021-05-10 15:10:48:INFO:Found local kubernetes config. Initialized with kube_config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-10 15:10:48:ERROR:Exception when calling CoreV1Api->create_namespaced_pod: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b28cee7b-b922-4769-ae24-83ce5de13f1a', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Mon, 10 May 2021 15:10:48 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\", line 65, in _create_k8s_job\n",
      "    api_response = self._corev1.create_namespaced_pod(yaml_spec['metadata']['namespace'], pod)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6174, in create_namespaced_pod\n",
      "    (data) = self.create_namespaced_pod_with_http_info(namespace, body, **kwargs)  # noqa: E501\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6265, in create_namespaced_pod_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n",
      "    _preload_content, _request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 388, in request\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 278, in POST\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n",
      "    raise ApiException(http_resp=r)\n",
      "kubernetes.client.rest.ApiException: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b28cee7b-b922-4769-ae24-83ce5de13f1a', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Mon, 10 May 2021 15:10:48 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Kubernetes job creation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e070fe765dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbase_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_IMAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdependency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionedDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scikit-learn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.21.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     target_image=PREPROCESS_IMG)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_component_builder.py\u001b[0m in \u001b[0;36mbuild_python_component\u001b[0;34m(component_func, target_image, base_image, dependency, staging_gcs_path, timeout, namespace, target_component_file, python_version, is_v2)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building and pushing container image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mcontainer_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContainerBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaging_gcs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mimage_name_with_digest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_build_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_docker_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_name_with_digest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_container_builder.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, local_dir, docker_filename, target_image, timeout)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_k8s_job_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mk8s_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mresult_pod_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkaniko_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kaniko job complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m(self, yaml_spec, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'namespace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kubernetes job creation failed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;31m# timeout in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0msucc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_k8s_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Kubernetes job creation failed."
     ]
    }
   ],
   "source": [
    "preprocessing_img_op = compiler.build_python_component(\n",
    "    component_func=preprocess,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='scikit-learn', version='0.21.2')],\n",
    "    target_image=PREPROCESS_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes the training captions from the previous step and tokenizes them to convert them into numerical values so that they can be fed into the model as input.  It outputs the tokenized captions in `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) python_component. (This decorator does not seem to be used, so we deprecate it. If you need this decorator, please create an issue at https://github.com/kubeflow/pipelines/issues) -- Deprecated since version 0.2.6.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "@dsl.python_component(\n",
    "    name='tokenize_captions',\n",
    "    description='Tokenize captions to create training data',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def tokenize_captions(dataset_path: str, preprocess_output: str, OUTPUT_DIR: str,\n",
    "        top_k: int) -> str:\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from io import BytesIO\n",
    "    from ast import literal_eval as make_tuple\n",
    "    \n",
    "    # Convert output from string to tuple and unpack\n",
    "    preprocess_output = make_tuple(preprocess_output)\n",
    "    train_caption_path = preprocess_output[0]\n",
    "    \n",
    "    if OUTPUT_DIR == 'default':\n",
    "        OUTPUT_DIR = dataset_path + '/tokenize/'\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "    f = BytesIO(file_io.read_file_to_string(train_caption_path, \n",
    "                                            binary_mode=True))\n",
    "    train_captions = np.load(f)\n",
    "    \n",
    "    # Tokenize captions\n",
    "    tokenizer.fit_on_texts(train_captions)\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    \n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "    \n",
    "    # Find the maximum length of any caption in our dataset\n",
    "    def calc_max_length(tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "    \n",
    "    max_length = calc_max_length(train_seqs)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_file_path = OUTPUT_DIR + 'tokenizer.pickle'\n",
    "    with file_io.FileIO(tokenizer_file_path, 'wb') as output:\n",
    "        pickle.dump(tokenizer, output, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # Save train_seqs\n",
    "    cap_vector_file_path = OUTPUT_DIR + 'cap_vector.npy'\n",
    "    np.save(file_io.FileIO(cap_vector_file_path, 'w'), cap_vector)\n",
    "    \n",
    "    return str(max_length), tokenizer_file_path, cap_vector_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:16:33:INFO:Build an image that is based on gcr.io/iassist-deepdreamers/img-cap:latest and push the image to gcr.io/iassist-deepdreamersms-coco/tokenize:latest\n",
      "2021-05-07 22:16:33:INFO:Building and pushing container image.\n",
      "2021-05-07 22:16:33:INFO:Generate build files.\n",
      "2021-05-07 22:16:33:INFO:Start a kaniko job for build.\n",
      "2021-05-07 22:16:33:INFO:Cannot find in-cluster config, trying the local kubernetes config. \n",
      "2021-05-07 22:16:33:INFO:Found local kubernetes config. Initialized with kube_config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:16:33:ERROR:Exception when calling CoreV1Api->create_namespaced_pod: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': '087a3130-7db2-4937-8b16-263ccfeb9e1c', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 May 2021 22:16:33 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\", line 65, in _create_k8s_job\n",
      "    api_response = self._corev1.create_namespaced_pod(yaml_spec['metadata']['namespace'], pod)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6174, in create_namespaced_pod\n",
      "    (data) = self.create_namespaced_pod_with_http_info(namespace, body, **kwargs)  # noqa: E501\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6265, in create_namespaced_pod_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n",
      "    _preload_content, _request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 388, in request\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 278, in POST\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n",
      "    raise ApiException(http_resp=r)\n",
      "kubernetes.client.rest.ApiException: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': '087a3130-7db2-4937-8b16-263ccfeb9e1c', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 May 2021 22:16:33 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Kubernetes job creation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-8eab63165bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstaging_gcs_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPELINE_STORAGE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbase_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_IMAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     target_image=TOKENIZE_IMG)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_component_builder.py\u001b[0m in \u001b[0;36mbuild_python_component\u001b[0;34m(component_func, target_image, base_image, dependency, staging_gcs_path, timeout, namespace, target_component_file, python_version, is_v2)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building and pushing container image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mcontainer_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContainerBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaging_gcs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mimage_name_with_digest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_build_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_docker_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_name_with_digest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_container_builder.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, local_dir, docker_filename, target_image, timeout)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_k8s_job_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mk8s_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mresult_pod_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkaniko_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kaniko job complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m(self, yaml_spec, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'namespace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kubernetes job creation failed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;31m# timeout in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0msucc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_k8s_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Kubernetes job creation failed."
     ]
    }
   ],
   "source": [
    "tokenize_captions_op = compiler.build_python_component(\n",
    "    component_func=tokenize_captions,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    target_image=TOKENIZE_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component for training model (and saving it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component trains the model by creating a `tf.data.Dataset` from the captions and preprocessed images.  The trained model is saved in `train_output_dir/checkpoints/`.  The training loss is plotted in tensorboard. There are various parameters of the model(s) that can be tuned, but it uses the values from the original notebook by default.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) python_component. (This decorator does not seem to be used, so we deprecate it. If you need this decorator, please create an issue at https://github.com/kubeflow/pipelines/issues) -- Deprecated since version 0.2.6.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "@dsl.python_component(\n",
    "    name='model_training',\n",
    "    description='Trains image captioning model',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def train_model(dataset_path: str, preprocess_output: str, \n",
    "        tokenizing_output: str, train_output_dir: str, valid_output_dir: str, \n",
    "        batch_size: int, embedding_dim: int, units: int, EPOCHS: int)-> str:\n",
    "    import json\n",
    "    import time\n",
    "    import pickle\n",
    "    import models\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from io import BytesIO\n",
    "    from datetime import datetime\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from ast import literal_eval as make_tuple\n",
    "    \n",
    "    # Convert output from string to tuple and unpack\n",
    "    preprocess_output = make_tuple(preprocess_output)\n",
    "    tokenizing_output = make_tuple(tokenizing_output)\n",
    "    \n",
    "    # Unpack tuples\n",
    "    preprocessed_imgs_path = preprocess_output[1]\n",
    "    tokenizer_path = tokenizing_output[1]\n",
    "    cap_vector_file_path = tokenizing_output[2]\n",
    "    \n",
    "    if valid_output_dir == 'default':\n",
    "        valid_output_dir = dataset_path + '/valid/'\n",
    "    \n",
    "    if train_output_dir == 'default':\n",
    "        train_output_dir = dataset_path + '/train/'\n",
    "    \n",
    "    # load img_name_vector\n",
    "    f = BytesIO(file_io.read_file_to_string(preprocessed_imgs_path, binary_mode=True))\n",
    "    img_name_vector = np.load(f)\n",
    "    \n",
    "    # Load cap_vector\n",
    "    f = BytesIO(file_io.read_file_to_string(cap_vector_file_path, binary_mode=True))\n",
    "    cap_vector = np.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with file_io.FileIO(tokenizer_path, 'rb') as src:\n",
    "        tokenizer = pickle.load(src)\n",
    "    \n",
    "    # Split data into training and testing\n",
    "    img_name_train, img_name_val, cap_train, cap_val = train_test_split(\n",
    "                                                            img_name_vector,\n",
    "                                                            cap_vector,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=0)\n",
    "    \n",
    "    # Create tf.data dataset for training\n",
    "    BUFFER_SIZE = 1000 # common size used for shuffling dataset\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    num_steps = len(img_name_train) // batch_size\n",
    "    \n",
    "    # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "    features_shape = 2048\n",
    "    \n",
    "    # Load the numpy files\n",
    "    def map_func(img_name, cap):\n",
    "        f = BytesIO(file_io.read_file_to_string(img_name.decode('utf-8'), binary_mode=True))\n",
    "        img_tensor = np.load(f)\n",
    "        return img_tensor, cap\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "    # Use map to load the numpy files in parallel\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "              map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Shuffle and batch\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # get models from models.py\n",
    "    encoder = models.CNN_Encoder(embedding_dim)\n",
    "    decoder = models.RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    \n",
    "    # Create loss function\n",
    "    def loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = loss_object(real, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "\n",
    "        return tf.reduce_mean(loss_)\n",
    "    \n",
    "    # Create check point for training model\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, train_output_dir + 'checkpoints/', max_to_keep=5)\n",
    "    start_epoch = 0\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "            \n",
    "    # Create training step\n",
    "    loss_plot = []\n",
    "    @tf.function\n",
    "    def train_step(img_tensor, target):\n",
    "        loss = 0\n",
    "\n",
    "        # initializing the hidden state for each batch\n",
    "        # because the captions are not related from image to image\n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = encoder(img_tensor)\n",
    "\n",
    "            for i in range(1, target.shape[1]):\n",
    "                # passing the features through the decoder\n",
    "                predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "                loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, total_loss\n",
    "    \n",
    "    # Create summary writers and loss for plotting loss in tensorboard\n",
    "    tensorboard_dir = train_output_dir + 'logs/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_summary_writer = tf.summary.create_file_writer(tensorboard_dir)\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    \n",
    "    # Train model\n",
    "    path_to_most_recent_ckpt = None\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss\n",
    "            train_loss(t_loss)\n",
    "            if batch % 100 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Storing the epoch end loss value to plot in tensorboard\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss per epoch', train_loss.result(), step=epoch)\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            path_to_most_recent_ckpt = ckpt_manager.save()\n",
    "\n",
    "        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                             total_loss/num_steps))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    # Add plot of loss in tensorboard\n",
    "    metadata ={\n",
    "        'outputs': [{\n",
    "            'type': 'tensorboard',\n",
    "            'source': tensorboard_dir,\n",
    "        }]\n",
    "    }\n",
    "    with open('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    # Save validation data to use for predictions\n",
    "    val_cap_path = valid_output_dir + 'captions.npy'\n",
    "    np.save(file_io.FileIO(val_cap_path, 'w'), cap_val)\n",
    "    \n",
    "    val_img_path = valid_output_dir + 'images.npy'\n",
    "    np.save(file_io.FileIO(val_img_path, 'w'), img_name_val)\n",
    "    \n",
    "    return path_to_most_recent_ckpt, val_cap_path, val_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:16:44:INFO:Build an image that is based on gcr.io/iassist-deepdreamers/img-cap:latest and push the image to gcr.io/iassist-deepdreamersms-coco/train:latest\n",
      "2021-05-07 22:16:44:INFO:Building and pushing container image.\n",
      "2021-05-07 22:16:44:INFO:Generate build files.\n",
      "2021-05-07 22:16:44:INFO:Start a kaniko job for build.\n",
      "2021-05-07 22:16:44:INFO:Cannot find in-cluster config, trying the local kubernetes config. \n",
      "2021-05-07 22:16:44:INFO:Found local kubernetes config. Initialized with kube_config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:16:44:ERROR:Exception when calling CoreV1Api->create_namespaced_pod: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': '5c764ecd-b493-48b4-93a8-78e6b5ffa6e2', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 May 2021 22:16:44 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\", line 65, in _create_k8s_job\n",
      "    api_response = self._corev1.create_namespaced_pod(yaml_spec['metadata']['namespace'], pod)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6174, in create_namespaced_pod\n",
      "    (data) = self.create_namespaced_pod_with_http_info(namespace, body, **kwargs)  # noqa: E501\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6265, in create_namespaced_pod_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n",
      "    _preload_content, _request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 388, in request\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 278, in POST\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n",
      "    raise ApiException(http_resp=r)\n",
      "kubernetes.client.rest.ApiException: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': '5c764ecd-b493-48b4-93a8-78e6b5ffa6e2', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 May 2021 22:16:44 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Kubernetes job creation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-49943ca5717b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbase_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_IMAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdependency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionedDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scikit-learn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.21.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     target_image=TRAIN_IMG)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_component_builder.py\u001b[0m in \u001b[0;36mbuild_python_component\u001b[0;34m(component_func, target_image, base_image, dependency, staging_gcs_path, timeout, namespace, target_component_file, python_version, is_v2)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building and pushing container image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mcontainer_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContainerBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaging_gcs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mimage_name_with_digest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_build_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_docker_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_name_with_digest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_container_builder.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, local_dir, docker_filename, target_image, timeout)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_k8s_job_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mk8s_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mresult_pod_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkaniko_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kaniko job complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m(self, yaml_spec, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'namespace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kubernetes job creation failed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;31m# timeout in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0msucc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_k8s_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Kubernetes job creation failed."
     ]
    }
   ],
   "source": [
    "model_train_op = compiler.build_python_component(\n",
    "    component_func=train_model,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='scikit-learn', version='0.21.2')],\n",
    "    target_image=TRAIN_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component for model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component uses the model to predict on a new image.  It prints the predicted and real caption in the logs and outputs the first 10 attention images with captions in tensorboard.  (Currently Kubeflow [only supports up to 10 outputs](https://github.com/kubeflow/pipelines/issues/1641) Tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) python_component. (This decorator does not seem to be used, so we deprecate it. If you need this decorator, please create an issue at https://github.com/kubeflow/pipelines/issues) -- Deprecated since version 0.2.6.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "@dsl.python_component(\n",
    "    name='model_predictions',\n",
    "    description='Predicts on images in validation set',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def predict(dataset_path: str, tokenizing_output: str, \n",
    "        model_train_output: str, preprocess_output_dir: str, \n",
    "        valid_output_dir: str, embedding_dim: int, units: int):\n",
    "    import pickle\n",
    "    import json\n",
    "    import models\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from datetime import datetime\n",
    "    from io import BytesIO\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    from ast import literal_eval as make_tuple\n",
    "    \n",
    "    tokenizing_output = make_tuple(tokenizing_output)\n",
    "    model_train_output = make_tuple(model_train_output)\n",
    "    \n",
    "    # Unpack tuples\n",
    "    max_length = int(tokenizing_output[0])\n",
    "    tokenizer_path = tokenizing_output[1]\n",
    "    model_path = model_train_output[0]\n",
    "    val_cap_path = model_train_output[1]\n",
    "    val_img_path = model_train_output[2]\n",
    "    \n",
    "    if preprocess_output_dir == 'default':\n",
    "        preprocess_output_dir = dataset_path + '/preprocess/'\n",
    "    \n",
    "    if valid_output_dir == 'default':\n",
    "        valid_output_dir = dataset_path + '/valid/'\n",
    "        \n",
    "    tensorboard_dir = valid_output_dir + 'logs' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    summary_writer = tf.summary.create_file_writer(tensorboard_dir)\n",
    "\n",
    "    # Load tokenizer, model, test_captions, and test_imgs\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with file_io.FileIO(tokenizer_path, 'rb') as src:\n",
    "        tokenizer = pickle.load(src)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "    attention_features_shape = 64\n",
    "    features_shape = 2048\n",
    "    \n",
    "    encoder = models.CNN_Encoder(embedding_dim)\n",
    "    decoder = models.RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "    \n",
    "    # Load model from checkpoint (encoder, decoder)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder, optimizer=optimizer)\n",
    "    ckpt.restore(model_path).expect_partial()\n",
    "    \n",
    "    # Load test captions\n",
    "    f = BytesIO(file_io.read_file_to_string(val_cap_path, \n",
    "                                            binary_mode=True))\n",
    "    cap_val = np.load(f)\n",
    "    \n",
    "    # load test images\n",
    "    f = BytesIO(file_io.read_file_to_string(val_img_path, \n",
    "                                            binary_mode=True))\n",
    "    img_name_val = np.load(f)\n",
    "    \n",
    "    # To get original image locations, replace .npy extension with .jpg and \n",
    "    # replace preprocessed path with path original images\n",
    "    PATH = dataset_path + '/train2014/train2014/'\n",
    "    img_name_val = [img.replace('.npy', '.jpg') for img in img_name_val]\n",
    "    img_name_val = [img.replace(preprocess_output_dir, PATH) for img in img_name_val]\n",
    "    \n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "    \n",
    "    # Preprocess the images using InceptionV3\n",
    "    def load_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path\n",
    "    \n",
    "    # Run predictions\n",
    "    def evaluate(image):\n",
    "        attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "        hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "        temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "        img_tensor_val = image_features_extract_model(temp_input)\n",
    "        img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "        features = encoder(img_tensor_val)\n",
    "\n",
    "        dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "        result = []\n",
    "\n",
    "        for i in range(max_length):\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "            if tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result, attention_plot\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        attention_plot = attention_plot[:len(result), :]\n",
    "        return result, attention_plot\n",
    "    \n",
    "    # Modified to plot images on tensorboard\n",
    "    def plot_attention(image, result, attention_plot):\n",
    "        img = tf.io.read_file(image)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        temp_image = np.array(img.numpy())\n",
    "        \n",
    "        len_result = len(result)\n",
    "        for l in range(min(len_result, 10)): # Tensorboard only supports 10 imgs\n",
    "            temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "            plt.title(result[l])\n",
    "            img = plt.imshow(temp_image)\n",
    "            plt.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "            \n",
    "            # Save plt to image to access in tensorboard\n",
    "            buf = BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            \n",
    "            final_im = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "            final_im = tf.expand_dims(final_im, 0)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.image(\"attention\", final_im, step=l)\n",
    "    \n",
    "    # Select a random image to caption from validation set\n",
    "    rid = np.random.randint(0, len(img_name_val))\n",
    "    image = img_name_val[rid]\n",
    "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "    result, attention_plot = evaluate(image)\n",
    "    print ('Image:', image)\n",
    "    print ('Real Caption:', real_caption)\n",
    "    print ('Prediction Caption:', ' '.join(result))\n",
    "    plot_attention(image, result, attention_plot)\n",
    "    \n",
    "    # Plot attention images on tensorboard\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'tensorboard',\n",
    "            'source': tensorboard_dir,\n",
    "        }]\n",
    "    }\n",
    "    with open('/mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:16:57:INFO:Build an image that is based on gcr.io/iassist-deepdreamers/img-cap:latest and push the image to gcr.ioiassist-deepdreamersms-coco/predict:latest\n",
      "2021-05-07 22:16:57:INFO:Building and pushing container image.\n",
      "2021-05-07 22:16:57:INFO:Generate build files.\n",
      "2021-05-07 22:16:58:INFO:Start a kaniko job for build.\n",
      "2021-05-07 22:16:58:INFO:Cannot find in-cluster config, trying the local kubernetes config. \n",
      "2021-05-07 22:16:58:INFO:Found local kubernetes config. Initialized with kube_config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:16:58:ERROR:Exception when calling CoreV1Api->create_namespaced_pod: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': '32e650ae-0130-450c-874d-bbb0a8354af4', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 May 2021 22:16:58 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\", line 65, in _create_k8s_job\n",
      "    api_response = self._corev1.create_namespaced_pod(yaml_spec['metadata']['namespace'], pod)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6174, in create_namespaced_pod\n",
      "    (data) = self.create_namespaced_pod_with_http_info(namespace, body, **kwargs)  # noqa: E501\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 6265, in create_namespaced_pod_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n",
      "    _preload_content, _request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 388, in request\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 278, in POST\n",
      "    body=body)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n",
      "    raise ApiException(http_resp=r)\n",
      "kubernetes.client.rest.ApiException: (404)\n",
      "Reason: Not Found\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': '32e650ae-0130-450c-874d-bbb0a8354af4', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 May 2021 22:16:58 GMT', 'Content-Length': '196'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"namespaces \\\"kubeflow\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"kubeflow\",\"kind\":\"namespaces\"},\"code\":404}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Kubernetes job creation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-d40c7bef1eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbase_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBASE_IMAGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdependency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionedDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3.1.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     target_image=PREDICT_IMG)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_component_builder.py\u001b[0m in \u001b[0;36mbuild_python_component\u001b[0;34m(component_func, target_image, base_image, dependency, staging_gcs_path, timeout, namespace, target_component_file, python_version, is_v2)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building and pushing container image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mcontainer_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContainerBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaging_gcs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mimage_name_with_digest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_build_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_docker_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_name_with_digest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_container_builder.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, local_dir, docker_filename, target_image, timeout)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_k8s_job_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mk8s_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK8sJobHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mresult_pod_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkaniko_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kaniko job complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/containers/_k8s_job_helper.py\u001b[0m in \u001b[0;36mrun_job\u001b[0;34m(self, yaml_spec, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'namespace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kubernetes job creation failed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;31m# timeout in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0msucc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_k8s_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Kubernetes job creation failed."
     ]
    }
   ],
   "source": [
    "predict_op = compiler.build_python_component(\n",
    "    component_func=predict,\n",
    "    staging_gcs_path=PIPELINE_STORAGE_PATH,\n",
    "    base_image=BASE_IMAGE,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='matplotlib', version='3.1.0')],\n",
    "    target_image=PREDICT_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run pipeline\n",
    "### Create pipeline\n",
    "The pipeline parameters are specified below in the `caption pipeline` function signature.  Using the value `'default'` for the output directories saves them in a subdirectory of `GCS_DATASET_PATH`.\n",
    "\n",
    "### Requirements\n",
    "* The pipeline can authenticate to GCP. Refer to [Authenticating Pipelines to GCP](https://www.kubeflow.org/docs/gke/authentication-pipelines/) for details.\n",
    "* Read/write permissions for the storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Image Captioning Pipeline',\n",
    "    description='A pipeline that trains a model to caption images'\n",
    ")\n",
    "def caption_pipeline(\n",
    "    dataset_path=GCS_DATASET_PATH,\n",
    "    num_examples=30000,\n",
    "    epochs=20,\n",
    "    training_batch_size=64,\n",
    "    hidden_state_size=512,\n",
    "    vocab_size=5000,\n",
    "    embedding_dim=256,\n",
    "    preprocessing_batch_size=16,\n",
    "    preprocessing_output_dir='default',\n",
    "    tokenizing_output_dir='default',\n",
    "    training_output_dir='default',\n",
    "    validation_output_dir='default',\n",
    "    ): \n",
    "    \n",
    "    preprocessing_img_task = preprocessing_img_op(\n",
    "        dataset_path, \n",
    "        output_dir=preprocessing_output_dir,\n",
    "        batch_size=preprocessing_batch_size, \n",
    "        num_examples=num_examples)\n",
    "    \n",
    "    tokenize_captions_task = tokenize_captions_op(\n",
    "        dataset_path, \n",
    "        preprocessing_img_task.output, \n",
    "        output_dir=tokenizing_output_dir, \n",
    "        top_k=vocab_size)\n",
    "    \n",
    "    model_train_task = model_train_op(\n",
    "        dataset_path, \n",
    "        preprocessing_img_task.output,\n",
    "        tokenize_captions_task.output,\n",
    "        train_output_dir=training_output_dir, \n",
    "        valid_output_dir=validation_output_dir,\n",
    "        batch_size=training_batch_size, \n",
    "        embedding_dim=embedding_dim, \n",
    "        units=hidden_state_size, \n",
    "        epochs=epochs)\n",
    "    \n",
    "    predict_task = predict_op(\n",
    "        dataset_path,\n",
    "        tokenize_captions_task.output, \n",
    "        model_train_task.output,\n",
    "        preprocess_output_dir=preprocessing_output_dir,\n",
    "        valid_output_dir=validation_output_dir,\n",
    "        embedding_dim=embedding_dim,\n",
    "        units=hidden_state_size)\n",
    "\n",
    "    # The pipeline should be able to authenticate to GCP.\n",
    "    # Refer to [Authenticating Pipelines to GCP](https://www.kubeflow.org/docs/gke/authentication-pipelines/) for details.\n",
    "    #\n",
    "    # For example, you may uncomment the following lines to use GSA keys.\n",
    "    # from kfp.gcp import use_gcp_secret\n",
    "    # kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run to make sure all parts of the pipeline are working properly\n",
    "arguments = {\n",
    "    'dataset_path': GCS_DATASET_PATH, \n",
    "    'num_examples': 100, # Small test to make sure pipeline functions properly\n",
    "    'training_batch_size': 16, # has to be smaller since only training on 80/100 examples \n",
    "}\n",
    "\n",
    "kfp.Client().create_run_from_pipeline_func(pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model checkpoints are saved at `training_output_dir`, which is `[GCS_DATASET_PATH]/train/checkpoints/` by default."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
